{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alamoodi-Abdullah/Data_Science-Projects-ML-/blob/main/LDA_TOPIC_MODEL_MCO1_1_2_Weeks_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIPgGmjZRcZf"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "One of the primary applications of natural language processing is to automatically extract what topics people are discussing from large volumes of text. Some examples of large text could be feeds from social media, customer reviews of hotels, movies, etc, user feedbacks, news stories, e-mails of customer complaints etc.\n",
        "Knowing what people are talking about and understanding their problems and opinions is highly valuable to businesses, administrators, political campaigns. And it’s really hard to manually read through such large volumes and compile the topics.\n",
        "\n",
        "Thus is required an automated algorithm that can read through the text documents and automatically output the topics discussed.\n",
        "\n",
        "In this tutorial, we will take a real example of the ’20 Newsgroups’ dataset and use LDA to extract the naturally discussed topics.\n",
        "\n",
        "I will be using the Latent Dirichlet Allocation (LDA) from Gensim package along with the Mallet’s implementation (via Gensim). Mallet has an efficient implementation of the LDA. It is known to run faster and gives better topics segregation.\n",
        "\n",
        "We will also extract the volume and percentage contribution of each topic to get an idea of how important a topic is.\n",
        "Let’s begin!\n",
        "\n",
        "Topic Modeling with Gensim in Python. Photo by Jeremy Bishop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z60Lf8UQRcZn"
      },
      "source": [
        "# 2. Import Packages\n",
        "The core packages used in this tutorial are re, gensim, spacy and pyLDAvis. Besides this we will also using matplotlib, numpy and pandas for data handling and visualization. Let’s import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApY7ujllSSU0"
      },
      "outputs": [],
      "source": [
        "#!pip install pyLDAvis\n",
        "#!pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHExlTKjRcZn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ynB-grCRcZo"
      },
      "source": [
        "# 3. Prerequisites – Download nltk stopwords and spacy model\n",
        "\n",
        "We will need the stopwords from NLTK and spacy’s en model for text pre-processing. Later, we will be using the spacy model for lemmatization.\n",
        "\n",
        "Lemmatization is nothing but converting a word to its root word. For example: the lemma of the word ‘machines’ is ‘machine’. Likewise, ‘walking’ –> ‘walk’, ‘mice’ –> ‘mouse’ and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNzS5VzeRcZp"
      },
      "source": [
        "# 4. What does LDA do?\n",
        "LDA’s approach to topic modeling is it considers each document as a collection of topics in a certain proportion. And each topic as a collection of keywords, again, in a certain proportion.\n",
        "\n",
        "Once you provide the algorithm with the number of topics, all it does it to rearrange the topics distribution within the documents and keywords distribution within the topics to obtain a good composition of topic-keywords distribution.\n",
        "\n",
        "When I say topic, what is it actually and how it is represented?\n",
        "A topic is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the keywords, you can identify what the topic is all about.\n",
        "\n",
        "The following are key factors to obtaining good segregation topics:\n",
        "\n",
        "The quality of text processing.\n",
        "The variety of topics the text talks about.\n",
        "The choice of topic modeling algorithm.\n",
        "The number of topics fed to the algorithm.\n",
        "The algorithms tuning parameters.\n",
        "# Prepare Stopwords\n",
        "We have already downloaded the stopwords. Let’s import them and make it available in stop_words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nad7OaM7Td4K"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meGKSTFtTVSl",
        "outputId": "272a8666-d3db-41a9-a53e-12e663d99866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PtI8F_lRcZp"
      },
      "outputs": [],
      "source": [
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e35j2a2dRcZq"
      },
      "source": [
        "# 6. Import Newsgroups Data\n",
        "We will be using the 20-Newsgroups dataset for this exercise. This version of the dataset contains about 11k newsgroups posts from 20 different topics. This is available as newsgroups.json.\n",
        "\n",
        "This is imported using pandas.read_json and the resulting dataset has 3 columns as shown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "qW-9RuK3RcZr",
        "outputId": "958a9db8-3d1d-43ef-c0ab-87cb44756871"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Ori_text</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>soon nys residents affected by coronavirus out...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>so far my isolation life is amazingly similar ...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>wfmy remaining open is proof that “essential “...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>stayathome with skittles</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>i guess a recovered  patient is very well posi...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                           Ori_text            id\n",
              "0           0  soon nys residents affected by coronavirus out...  1.245093e+18\n",
              "1           1  so far my isolation life is amazingly similar ...  1.245093e+18\n",
              "2           2  wfmy remaining open is proof that “essential “...  1.245093e+18\n",
              "3           3                         stayathome with skittles    1.245093e+18\n",
              "4           4  i guess a recovered  patient is very well posi...  1.245093e+18"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import Dataset\n",
        "df = pd.read_csv(\"MCO11colab.csv\", delimiter=',')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NLs56n-RcZr"
      },
      "outputs": [],
      "source": [
        "#df = df.iloc[0:50, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxx3M5paRcZs",
        "outputId": "0c52bf35-5226-4a74-8188-65c64a8aef51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(486119, 3)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "EgINkCc6RcZt",
        "outputId": "479a3f2c-f2a4-4243-acdd-580eaac4b0f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>content</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>soon nys residents affected by coronavirus out...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>so far my isolation life is amazingly similar ...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>wfmy remaining open is proof that “essential “...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>stayathome with skittles</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>i guess a recovered  patient is very well posi...</td>\n",
              "      <td>1.245093e+18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                            content            id\n",
              "0           0  soon nys residents affected by coronavirus out...  1.245093e+18\n",
              "1           1  so far my isolation life is amazingly similar ...  1.245093e+18\n",
              "2           2  wfmy remaining open is proof that “essential “...  1.245093e+18\n",
              "3           3                         stayathome with skittles    1.245093e+18\n",
              "4           4  i guess a recovered  patient is very well posi...  1.245093e+18"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.rename(columns={'Ori_text': 'content'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30DjqSTtRcZt"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX4TuQ_dRcZt"
      },
      "source": [
        "# 7. Remove emails and newline characters\n",
        "As you can see there are many emails, newline and extra spaces that is quite distracting. Let’s get rid of them using regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyao8ZzzRcZu",
        "outputId": "f9f30783-ee27-428c-b941-70dcb47ee2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['soon nys residents affected by coronavirus outbreak can get financial relief '\n",
            " 'by deferring mortgage payments for  to some lenders also the nysdfs will now '\n",
            " 'waive many bank fees we all love to hate to learn more visit   stayathome '\n",
            " 'newyorktough  ']\n"
          ]
        }
      ],
      "source": [
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "#data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "#data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "#data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "\n",
        "pprint(data[:1])"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "gcMgBeLsRcZu"
      },
      "source": [
        "After removing the emails and extra spaces, the text still looks messy. It is not ready for the LDA to consume. You need to break down each sentence into a list of words through tokenization, while clearing up all the messy text in the process.\n",
        "\n",
        "Gensim’s simple_preprocess is great for this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3_iDITRcZu"
      },
      "source": [
        "# 8. Tokenize words and Clean-up text\n",
        "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
        "\n",
        "Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEts78DZRcZu",
        "outputId": "f9b55d66-4b96-4002-b62d-7d555309d88f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['soon', 'nys', 'residents', 'affected', 'by', 'coronavirus', 'outbreak', 'can', 'get', 'financial', 'relief', 'by', 'deferring', 'mortgage', 'payments', 'for', 'to', 'some', 'lenders', 'also', 'the', 'nysdfs', 'will', 'now', 'waive', 'many', 'bank', 'fees', 'we', 'all', 'love', 'to', 'hate', 'to', 'learn', 'more', 'visit', 'stayathome', 'newyorktough']]\n"
          ]
        }
      ],
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZOpYuxWRcZv"
      },
      "source": [
        "# 9. Creating Bigram and Trigram Models\n",
        "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
        "\n",
        "Some examples in our example are: ‘front_bumper’, ‘oil_leak’, ‘maryland_college_park’ etc.\n",
        "\n",
        "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVNTVaWTRcZv",
        "outputId": "71f6d789-0156-427b-eae5-3bf80177402b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['soon', 'nys', 'residents', 'affected', 'by', 'coronavirus', 'outbreak', 'can', 'get', 'financial_relief', 'by', 'deferring', 'mortgage_payments', 'for', 'to', 'some', 'lenders', 'also', 'the', 'nysdfs', 'will', 'now', 'waive', 'many', 'bank', 'fees', 'we', 'all', 'love', 'to', 'hate', 'to', 'learn', 'more', 'visit', 'stayathome', 'newyorktough']\n"
          ]
        }
      ],
      "source": [
        "# Build the bigram and trigram models\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "\n",
        "# Faster way to get a sentence clubbed as a trigram/bigram\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# See trigram example\n",
        "print(trigram_mod[bigram_mod[data_words[0]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ievBHavmRcZv"
      },
      "source": [
        "# 10. Remove Stopwords, Make Bigrams and Lemmatize\n",
        "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6bTupr7ORcZv"
      },
      "outputs": [],
      "source": [
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O_wfO62rRcZw",
        "outputId": "87126748-e6de-4158-f72e-fac6439f26fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['soon', 'nys', 'resident', 'affect', 'get', 'financial', 'relief', 'defer', 'lender', 'also', 'nysdf', 'waive', 'many', 'bank', 'fee', 'love', 'hate', 'learn', 'visit', 'stayathome', 'newyorktough']]\n"
          ]
        }
      ],
      "source": [
        "# Remove Stop Words\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Form Bigrams\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# python3 -m spacy download en\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Do lemmatization keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKO_dC3mRcZw"
      },
      "source": [
        "# 11. Create the Dictionary and Corpus needed for Topic Modeling\n",
        "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8-3Ba6ULRcZw",
        "outputId": "b7390b7c-789e-485f-b5f4-4e7854f79cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)]]\n"
          ]
        }
      ],
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvsXQLLURcZw"
      },
      "source": [
        "Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency).\n",
        "\n",
        "For example, (0, 1) above implies, word id 0 occurs once in the first document. Likewise, word id 1 occurs twice and so on.\n",
        "\n",
        "This is used as the input by the LDA model.\n",
        "\n",
        "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEddFsP-RcZx"
      },
      "source": [
        "Or, you can see a human-readable form of the corpus itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AcExGL_uRcZx",
        "outputId": "1e93ba92-9c19-4067-e4b4-a962ce963dba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('affect', 1),\n",
              "  ('also', 1),\n",
              "  ('bank', 1),\n",
              "  ('defer', 1),\n",
              "  ('fee', 1),\n",
              "  ('financial', 1),\n",
              "  ('get', 1),\n",
              "  ('hate', 1),\n",
              "  ('learn', 1),\n",
              "  ('lender', 1),\n",
              "  ('love', 1),\n",
              "  ('many', 1),\n",
              "  ('newyorktough', 1),\n",
              "  ('nys', 1),\n",
              "  ('nysdf', 1),\n",
              "  ('relief', 1),\n",
              "  ('resident', 1),\n",
              "  ('soon', 1),\n",
              "  ('stayathome', 1),\n",
              "  ('visit', 1),\n",
              "  ('waive', 1)]]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Human readable format of corpus (term-frequency)\n",
        "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNj-1ZyIRcZx"
      },
      "source": [
        "# 12. Building the Topic Model\n",
        "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
        "\n",
        "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
        "\n",
        "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv3GmNI8RcZx"
      },
      "outputs": [],
      "source": [
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=38,\n",
        "                                           random_state=50,\n",
        "                                           update_every=20,\n",
        "                                           chunksize=50,\n",
        "                                           passes=10,\n",
        "                                           alpha='auto',\n",
        "                                           per_word_topics=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N3ScfkK5RcZx"
      },
      "outputs": [],
      "source": [
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Nj7pR5cB4EwC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vYF9j3hqtXEr"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "from gensim import models\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models import CoherenceModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdbFcALouRPc"
      },
      "outputs": [],
      "source": [
        "from gensim.models.wrappers import LdaMallet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT0QMuHLRcZx"
      },
      "outputs": [],
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCSLkNVnRcZy"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
        "#vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=10)\n",
        "pyLDAvis.save_html(vis, 'MCOs34.html')\n",
        "vis\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crj-lPKnRcZy"
      },
      "outputs": [],
      "source": [
        "#Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "#mallet_path = 'C:\\\\Mallet\\\\bin\\\\mallet'\n",
        "#ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwgkCaKFXZgo"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade gensim==3.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSOyh-RQXf3j"
      },
      "outputs": [],
      "source": [
        "import os       #importing os to set environment variable\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r6NT-8VXrPC"
      },
      "outputs": [],
      "source": [
        " #!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        " #!unzip mallet-2.0.8.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxNg2fXbXvTP"
      },
      "outputs": [],
      "source": [
        "#import zipfile\n",
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd9qQlhCXvv4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G31yE45kXUiI"
      },
      "outputs": [],
      "source": [
        "os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'\n",
        "mallet_path = '/content/mallet-2.0.8/bin/mallet' # you should NOT need to change this\n",
        "#corpus_path = 'transcripts' # you need to change this path to the directory containing your corpus of .txt files\n",
        "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=38, id2word=id2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpLmel6VXv6S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp3Vj8YuRcZy"
      },
      "outputs": [],
      "source": [
        "# # Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "# import os\n",
        "\n",
        "# os.environ.update({'MALLET_HOME': r'C:/mallet/'})\n",
        "# mallet_path = r'C:/mallet/bin/mallet.bat'\n",
        "# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=38, id2word=id2word)\n",
        "\n",
        "# #pprint(ldamallet.show_topics(formatted=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSyudPT_RcZy"
      },
      "outputs": [],
      "source": [
        "pprint(ldamallet.show_topics(formatted=False))\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_ldamallet)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "YBBOehYDRcZy"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKJNVrzyRcZy"
      },
      "outputs": [],
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit=40, start=2, step=6):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKRwyP6TRcZz"
      },
      "outputs": [],
      "source": [
        "# Can take a long time to run.\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5XaP5y9RcZz"
      },
      "outputs": [],
      "source": [
        "# Show graph\n",
        "limit=40; start=2; step=6;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence Score\")\n",
        "plt.axhline(0.30, c=(.7, .7, .7), ls='--')\n",
        "plt.axhline(0.35, c=(.7, .7, .7), ls='--')\n",
        "plt.axhline(0.40, c=(.7, .7, .7), ls='--')\n",
        "plt.axhline(0.45, c=(.7, .7, .7), ls='--')\n",
        "plt.axhline(0.50, c=(.7, .7, .7), ls='--')\n",
        "#plt.axhline(0.35, c=(.7, .7, .7), ls='--')\n",
        "#plt.axhline(0.34, c=(.7, .7, .7), ls='--')\n",
        "plt.axvline(5, c=(.7, .7, .7), ls='--')\n",
        "plt.axvline(10, c=(.7, .7, .7), ls='--')\n",
        "plt.axvline(15, c=(.7, .7, .7), ls='--')\n",
        "plt.axvline(20, c=(.7, .7, .7), ls='--')\n",
        "plt.axvline(25, c=(.7, .7, .7), ls='--')\n",
        "plt.axvline(30, c=(.7, .7, .7), ls='--')\n",
        "plt.axvline(35, c=(.7, .7, .7), ls='--')\n",
        "#plt.axvline(40, c=(.7, .7, .7), ls='--')\n",
        "#plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIyxV3SjRcZz"
      },
      "outputs": [],
      "source": [
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Number of Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIuKLuj4RcZz"
      },
      "outputs": [],
      "source": [
        "# Select the model and print the topics\n",
        "optimal_model = model_list[4]\n",
        "model_topics = optimal_model.show_topics(formatted=False)\n",
        "pprint(optimal_model.print_topics(num_words=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOeOM5MQRcZz"
      },
      "outputs": [],
      "source": [
        "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,40), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "df_dominant_topic.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_oZu6eeRcZ0"
      },
      "outputs": [],
      "source": [
        "df_dominant_topic.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9A8luk2RcZ0"
      },
      "outputs": [],
      "source": [
        "df_dominant_topic.to_csv('MCO11_Dominant_W1-2_Topics.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh8ykJNQRcZ0"
      },
      "outputs": [],
      "source": [
        "# Group top 5 sentences under each topic\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,\n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)],\n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index\n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySw4_wVjRcZ0"
      },
      "outputs": [],
      "source": [
        "# Number of Documents for Each Topic\n",
        "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
        "\n",
        "# Percentage of Documents for Each Topic\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
        "\n",
        "# Topic Number and Keywords\n",
        "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
        "\n",
        "# Concatenate Column wise\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
        "\n",
        "# Change Column names\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
        "\n",
        "# Show\n",
        "df_dominant_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI9BqIuBRcZ0"
      },
      "outputs": [],
      "source": [
        "df_dominant_topics.to_csv('MCO11_W1-2_Topics.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WG8eX7fRcZ0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}